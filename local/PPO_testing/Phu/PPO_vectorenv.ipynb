{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "addc1dae-c3f4-4866-9117-eb5252dd9694",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "# import Lorenz_envs\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import warnings\n",
    "import copy \n",
    "import wandb\n",
    "import le_envs\n",
    "\n",
    "from IPython import display\n",
    "from torch.distributions import MultivariateNormal\n",
    "from torch.optim import Adam\n",
    "from torch.nn import Linear, Module, MSELoss, ReLU, Sequential\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1256ed5b-22ee-4ec1-8288-2fbf1f21ba18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.model = Sequential(\n",
    "            Linear(input_dim, 64),\n",
    "            ReLU(),\n",
    "            Linear(64, 64),\n",
    "            ReLU(),\n",
    "            Linear(64, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, observation):\n",
    "        if isinstance(observation, np.ndarray):\n",
    "            observation = torch.tensor(observation, dtype=torch.float)\n",
    "        return self.model(observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44311e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardStrategy:\n",
    "    def __init__(self, gamma, gae_lambda):\n",
    "        self.gamma = gamma\n",
    "        self.gae_lambda = gae_lambda\n",
    "\n",
    "class RewardsToGo(RewardStrategy):\n",
    "    def __init__(self, gamma, gae_lambda):\n",
    "        super().__init__(gamma, gae_lambda)\n",
    "\n",
    "    def compute_advantages(self, rewards, values, masks):\n",
    "        batch_rtgs = []\n",
    "        for episode_rewards in reversed(rewards):\n",
    "            discounted_reward = 0\n",
    "            for reward in reversed(episode_rewards):\n",
    "                discounted_reward = reward + discounted_reward * self.gamma\n",
    "                batch_rtgs.insert(0, discounted_reward)\n",
    "        \n",
    "        batch_rtgs = torch.tensor(batch_rtgs, dtype=torch.float)\n",
    "        adv = batch_rtgs - values\n",
    "        return batch_rtgs, adv\n",
    "\n",
    "class GeneralizedAdvantage(RewardStrategy):\n",
    "    def __init__(self, gamma, gae_lambda):\n",
    "        super().__init__(gamma, gae_lambda)\n",
    "        self.coeff = gamma * gae_lambda\n",
    "\n",
    "    def compute_advantages(self, rewards, values, masks):\n",
    "        flat_rewards = torch.flatten(rewards)\n",
    "        batch_size = len(flat_rewards)\n",
    "        advantage = np.zeros(batch_size + 1)\n",
    "        advantage[batch_size - 1] = flat_rewards[batch_size - 1] - values[batch_size - 1]\n",
    "\n",
    "        for i in reversed(range(batch_size - 1)):\n",
    "            delta = flat_rewards[i] + (masks[i] * self.gamma * values[i + 1]) - values[i]\n",
    "            advantage[i] = delta + (masks[i] * self.coeff * advantage[i + 1])\n",
    "        \n",
    "        advantage = torch.tensor(advantage[:batch_size])\n",
    "        batch_returns = advantage + np.squeeze(values)\n",
    "        batch_returns = torch.tensor(batch_returns, dtype=torch.float)\n",
    "        return batch_returns, advantage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36bc19c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Objective:\n",
    "    def __init__(self, epsilon):\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "class SurrogateObjectiveClip(Objective):\n",
    "    def __init__(self, epsilon):\n",
    "        super().__init__(epsilon)\n",
    "\n",
    "    def get_loss(self, ratios, advantage):\n",
    "        surrogate_loss = ratios * advantage\n",
    "        clipped_surrogate_loss = torch.clamp(ratios, 1 - self.epsilon, 1 + self.epsilon) * advantage\n",
    "\n",
    "        return (-torch.min(surrogate_loss, clipped_surrogate_loss)).mean()\n",
    "\n",
    "class SurrogateObjectiveNoClip(Objective):\n",
    "    def __init__(self, epsilon):\n",
    "        super().__init__(epsilon)\n",
    "\n",
    "    def get_loss(self, ratios, advantage):\n",
    "        surrogate_loss = ratios * advantage\n",
    "\n",
    "        return (-torch.min(surrogate_loss)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b2e4fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen_env",
   "language": "python",
   "name": "gen_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
